---
title: "LLM Eval: Benchmarking Suite for Large Language Models"
excerpt: "Benchmarking and evaluation suite for large language models"
collection: portfolio
date: 2024-05-25 20:30:00 -0500
last_modified_at: 2024-06-07 08:30:00 -0500
---

A Python package for benchmarking and evaluating large language models (LLMs) on a standard benchmark datasets.
Benchmark parameters can be set using a JSON configuration file. The package supports benchmarking on multiple datasets
and evaluation metrics, and is designed to be easily extensible to add new datasets and evaluation metrics.

[GitHub](https://github.com/UMass-Meta-LLM-Eval/llm_eval)
