---
title: "GAN Implementation"
excerpt: "Implementing a basic GAN in Tensorflow"
header:
  teaser: /assets/projects/kaggle/disaster-tweets/teaser.jpg
sidebar:
  - title: "Competition"
    image: /assets/projects/kaggle/disaster-tweets/sidebar.jpg
    image_alt: "logo"
    text: "[Kaggle Competition](https://www.kaggle.com/c/nlp-getting-started)"
  - title: Deadline
    text: None
  - title: Prize Pool
    text: None
twitter-image: /assets/projects/kaggle/disaster-tweets/twitter-card.jpg
date: 2021-08-20 20:30:00 +0530
last_modified_at: 2021-12-23 20:30:00 +0530

dataset-samples:
  - url: /assets/projects/gan/dse1.png
    image_path: /assets/projects/gan/dse1.png
    alt: "Sample image from dataset"
    title: "Sample image. Label: Paper"
  - url: /assets/projects/gan/dse2.png
    image_path: /assets/projects/gan/dse2.png
    alt: "Sample image from dataset"
    title: "Sample image. Label: Paper"
  - url: /assets/projects/gan/dse3.png
    image_path: /assets/projects/gan/dse3.png
    alt: "Sample image from dataset"
    title: "Sample image. Label: Scissors"
  - url: /assets/projects/gan/dse4.png
    image_path: /assets/projects/gan/dse4.png
    alt: "Sample image from dataset"
    title: "Sample image. Label: Scissors"
  - url: /assets/projects/gan/dse5.png
    image_path: /assets/projects/gan/dse5.png
    alt: "Sample image from dataset"
    title: "Sample image. Label: Rock"
  - url: /assets/projects/gan/dse6.png
    image_path: /assets/projects/gan/dse6.png
    alt: "Sample image from dataset"
    title: "Sample image. Label: Rock"
---


# Introduction

GANs or Generative Adversarial Networks are generative models that try to learn the distribution of a given dataset using
two neural networks that work against each other while training. You can learn more about them here.

# Dataset

In this example we will use the [Rock Paper Scissors][1] dataset. This dataset contains about 2500 RGB CGI images of hands in
the "stone", "paper" or "scissors" positions with a 300x300 pixel resolution. Some example images are shown below:

{% include gallery id="dataset-samples" caption="Sample images from the dataset. Click to enlarge images." %}

## Image resizing

To achieve good performance with such a small dataset and limited compute power, it would be a good idea to make the images
grayscale and resize them to a more manageable resolution (for instance 100x100). But I wanted to see how far can we push this
method with limited dataset sizes so I used the original size images.

# Model

As explained above, the model consists of two neural networks, which are called the generator and the discriminator. The
generator tries to generate new points that hopefully look like they were drawn from the original dataset, while the discriminator
tries to distinguish between the real data points and the fake ones created by the generator.

## Discriminator

This one is relatively simple. Given a bunch of images, it needs to identify which ones are from the original dataset and which ones
were created by the generator, i.e., this is a binary classification model. It takes images of size 300x300x3 and produces a single 
scalar output which is the model's prediction of whether the given image is real or fake (crated by generator).

In our case, the model is a pretty standard CNN, with a bunch of 2D convolutional and max-pooling layers with leaky-ReLU activation and
dropout layers for regularization.

## Generator

The generator is in some sense the mirror image of the discriminator. To create a new image, the model needs a random 'noise' vector.
This noise vector acts as a random seed for the generator. From there, there are a bunch of 2D transpose-convolutional layers with leaky-ReLU
activation and batch-normalization until we end up with and image of the same resolution as the training dataset. Hopefully, the contents of
the dataset are also reflected in the images created by our generator.

# Steps

## Loss

To start the training process, we need to define the loss functions for both the generator and the discriminator.

For the generator, we want the images generated by it to appear real to the discriminator, thus we want the prediction for the generated images
from the discriminator to be close to 1. So, if the prediction by the generator is *pred*, we can set the loss as:

Lgen = cross_entropy(1, *pred*)

For the discriminator, we want it to correctly predict the real and fake images. Thus, the loss becomes:

Ldisc = cross_entropy(*target*, *pred*);

where,

*target* = 1 if image is real

*target* = 0 if image is fake

We sum up the losses for all the images in the batch to get the batch loss.

## Optimizer

For both the generator and the discriminator, we use separate instances of the Adam optimizer.

## Train step

One train step involves two parts:

  1. Create a batch of images using the generator and randomly created noise vectors, and get the predictions for them using the discriminator.
  Calculate the loss for the generator and update the weights of the generator while keeping the weights of the discriminator constant.
  2. Get a batch of images from the dataset and get the predictions for them using the discriminator. Use both the predictions on the real and the
  fake images to calculate the loss for the discriminator. Update the weights for the discriminator while keeping the generator weights constant.

Repeating these for multiple passes over the dataset should hopefully teach the generator what the images in the original dataset look like through 
the predictions made by the discriminator.

# Results

Training a GAN can be a little tricky, and the process can easily collapse giving meaningless results. After a few tries, I got these results from my model:

(Images here)

# Possible improvements

Many techniques can be applied to improve the model performance like:

 1. Increasing the size of the dataset - by data augmentation or generating more data points (which is possible in this case since the images are created by CGI)
 2. Progressively growing the size of the images during training. See StyleGAN 2
 3. Using different loss functions

[1]: <https://laurencemoroney.com/datasets.html#rock-paper-scissors-dataset> "Rock Paper Scissors Dataset - laurencemoroney.com"

