---
layout: single
title: "Inception Score"
twitter-image: /assets/images/default-teaser.jpg
excerpt: "Inception Score is a metric to measure the quality of images generated by GANs and other generative models."
header:
  teaser: /assets/images/default-teaser.jpg
  overlay_image: /assets/images/sample-img-wide.jpg
  overlay_filter: 0.5 # rgba(255, 0, 0, 0.5), linear-gradient(rgba(255, 0, 0, 0.5), rgba(0, 255, 255, 0.5))
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"

date:   2021-06-05 20:30:00 +0530
last_modified_at: 2021-06-09 20:30:00 +0530
categories: ML GAN
published: true

# table
toc: true
# toc_label: "Label goes here"
# toc_icon: "<some font awesome icon>"
toc_sticky: true

# sidebar
sidebar:
  - title: "Generative Adversarial Networks"
    image: /assets/images/ml/gan-sidebar.png
    image_alt: "Images generated by GANs"
    text: "GANs are generative models that can synthesize realistic images similar to the ones they are trained on."
  - nav: gans
---

# TL;DR

Inception Score is used to assess the quality of images produced by GANs. It uses the Inception V3 model to classify images (x) into classes (y) and then captures the fidelity and diversity of images by calculating the KL divergence between p(y\|x) and p(y):

$$ D_{KL}(p(y|x)||p(y)) = \int_{-\infty}^{\infty} p(y|x)log(\frac{p(y|x)}{p(y)}) \,dy $$

Finally, it is exponentiated to make the result easier to understand and compare:

 $$ IS = e^{\mathbb{E}_xD_{KL}(p(y|x)||p(y))} $$


# Introduction

The basic idea behind the Inception Score is that if a state-of-the-art image classifier thinks the images generated by your model are real, then they must be good.

To calculate the Inception Score, we use the Inception-V3 model to classify the generated images from our GAN, if the images are being classified into one of the classes with high confidence, then that is an indicator that the generated images are real looking. On the other hand, if the classifier is confused and can’t classify the generated images confidently, then perhaps the generated images are bad/low fidelity,

A good GAN will generate a large variety of real-looking images. Thus, the metric we use to assess the quality of the images should look at both of these factors - high fidelity and diversity.


# High fidelity

Looking at fidelity, for a given image x, the model predicts the distribution p(y\|x) over the possible classes y. We want this distribution to have a high value for a few classes (or ideally just one class) and very low for others, which would suggest that the model is very confident in classifying the image, while for a bad/low fidelity image we might expect the classifier to not be very sure about what is in the image and thus give a distribution closer to a uniform distribution. Another way of saying the same thing would be that we want the entropy of p(y\|x) to be as low as possible. 

Here entropy means the uncertainty or randomness in the outcome of the random variable. A well-made fake image of a dog is very likely to be predicted as a dog, so there will be less uncertainty in the outcome, and thus, low entropy.


# Diversity

It is also important that the model produces a diverse set of images, and we want the generated images to be more or less uniformly distributed across all classes, another way of thinking about it is that without seeing the generated image, we should have no idea what the classifier would predict, and thus the entropy of p(y) should be high. Notice that this time we didn’t condition it on a given x.

So, to summarize, we want the entropy of the distribution p(y\|x) to be low and that of p(y) to be high. These two things signify different aspects of what makes a GAN good. Think about how in a classifier, the precision and recall signify different aspects of the classifier. We can combine the two to create the F1 score so that we have one scalar number that can tell us how good the classifier is, it is not perfect but it gets the job done with just one number (Note: precision and recall can also be used to evaluate a GAN’s performance, which we will look into in another post). Similarly, when we combine the two values we described above, we get the Inception Score.

{% include figure image_path="/assets/posts/inception-score/fidelity-diversity.png" alt="Fidelity and diversity" caption="Tradeoff between high fidelity and diversity in GANs. Source: [Build Better Generative Adversarial Networks (GANs) \| Coursera](https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans/home/welcome)" %}


# Calculating the Inception Score

To capture the fidelity and diversity in a single number, we want to calculate the dissimilarity or distance between the two distributions, in the form of KL divergence, also called relative entropy.

$$ D_{KL}(p(y|x)||p(y)) = \int_{-\infty}^{\infty} p(y|x)log(\frac{p(y|x)}{p(y)}) \,dy $$

Note that while we can think of KL divergence as distance, strictly speaking, it is not a measure of distance because it is not symmetric, i.e., $ D_{KL}(P\|\|Q) \neq D_{KL}(Q\|\|P) $ in general. Decreasing the entropy of p(y\|x) or increasing the entropy of p(y) would both result in the KL divergence going up.

Now, we can average over the generated images x by taking the expectation. Finally, we take the exponent of this number to make the result easier to compare. So the complete formula then becomes:

 $$ IS = e^{\mathbb{E}_xD_{KL}(p(y|x)||p(y))} $$

You can check out [this blog post by Jason Brownlee](https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/) for the Python implementation of the IS.


# Limitations

While the Inception Score takes into consideration the diversity of classes in the images produced, it does not look at diversity within a particular class, so this method will still give us a good score if our model creates just one very realistic image in each class.

Also, similar to FID, it uses the Inception model which is trained on the ImageNet dataset, so if we are trying to generate images that are different from the kind of images found in that dataset, scores given by the classifier might not be very useful. It is entirely possible that a real-looking image doesn’t get a good prediction from the inception classifier, or conversely, a good score is given for a fake-looking image.

Another important issue is that the Inception Score doesn’t look at any real images to compare against, so it provides no information about how close our generated images are to the kind of images that we want to generate.


# References

[Build Better Generative Adversarial Networks (GANs) \| Coursera](https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans/home/welcome)

[[1606.03498] Improved Techniques for Training GANs (arxiv.org)](https://arxiv.org/abs/1606.03498)

[How to Implement the Inception Score (IS) for Evaluating GANs (machinelearningmastery.com)](https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/)

[Kullback–Leibler divergence - Wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)




